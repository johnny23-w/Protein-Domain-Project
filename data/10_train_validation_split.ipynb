{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_domain(dom_str):\n",
    "\n",
    "    def sort_domains(domain_list):\n",
    "        idx = sorted(range(len(domain_list)), key=lambda k: domain_list[k][1][0])\n",
    "        return [domain_list[i] for i in idx]\n",
    "\n",
    "    unsorted = [[it[0], list(map(int, it[1].split(':')))] for it in (dom.split(';') for dom in dom_str.split('|'))]\n",
    "\n",
    "    return sort_domains(unsorted)\n",
    "\n",
    "\n",
    "def count_real_domains(dom_list):\n",
    "    return len([dom for dom in dom_list if dom[0][0] not in ['m', 'd']])\n",
    "\n",
    "\n",
    "def train_seq_redundant(dom_list, test_doms):\n",
    "    return set([dom[0] for dom in dom_list]).isdisjoint(test_doms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16916\n"
     ]
    }
   ],
   "source": [
    "all_doms = []\n",
    "with open('non_test_nr.csv') as f:\n",
    "    for line in f:\n",
    "        all_doms += [dom[0] for dom in parse_domain(line.split(',')[1]) if dom[0]!='d' and dom[0][0]!='m']\n",
    "all_doms_counter = Counter(all_doms)\n",
    "print(len(all_doms_counter))\n",
    "most_common_doms = all_doms_counter.most_common(2000)\n",
    "most_common_doms = [dom for dom, _ in most_common_doms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('non_test_nr.csv') as f:\n",
    "    with open('non_valid.csv', 'w') as fNonValid:\n",
    "        with open('non_test_nr_no_common.csv', 'w') as fNoCom:\n",
    "            for line in f:\n",
    "                if set([dom[0] for dom in parse_domain(line.split(',')[1]) if dom[0]!='d']).isdisjoint(most_common_doms):\n",
    "                    fNoCom.write(line)\n",
    "                else:\n",
    "                    fNonValid.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Line 4000000\n"
     ]
    }
   ],
   "source": [
    "with open('non_test_nr_no_common.csv') as f:\n",
    "    with open('non_test_nr_sing_no_idr.csv', 'w') as fSingND:\n",
    "        with open('non_test_nr_multi_no_idr.csv', 'w') as fMultiND:\n",
    "            with open('non_test_nr_sing_idr.csv', 'w') as fSingID:\n",
    "                with open('non_test_nr_multi_idr.csv', 'w') as fMultiID:\n",
    "                    for idx, line in enumerate(f):\n",
    "                        doms = [dom[0] for dom in parse_domain(line.split(',')[1])]\n",
    "                        if 'd' not in doms:\n",
    "                            if count_real_domains(parse_domain(line.split(',')[1])) == 1:\n",
    "                                fSingND.write(line)\n",
    "                            else:\n",
    "                                fMultiND.write(line)\n",
    "                        else:\n",
    "                            if count_real_domains(parse_domain(line.split(',')[1])) == 1:\n",
    "                                fSingID.write(line)\n",
    "                            else:\n",
    "                                fMultiID.write(line)\n",
    "                        if idx%1000000==0:\n",
    "                            clear_output()\n",
    "                            print(f'Processing Line {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3897594\n",
      "68158\n",
      "118863\n",
      "5081\n"
     ]
    }
   ],
   "source": [
    "with open('non_test_nr_sing_no_idr.csv') as f:\n",
    "    sing_no_idr_counter = 0\n",
    "    for line in f:\n",
    "        sing_no_idr_counter += 1\n",
    "print(sing_no_idr_counter)\n",
    "with open('non_test_nr_multi_no_idr.csv') as f:\n",
    "    mult_no_idr_counter = 0\n",
    "    for line in f:\n",
    "        mult_no_idr_counter += 1\n",
    "print(mult_no_idr_counter)\n",
    "with open('non_test_nr_sing_idr.csv') as f:\n",
    "    sing_idr_counter = 0\n",
    "    for line in f:\n",
    "        sing_idr_counter += 1\n",
    "print(sing_idr_counter)\n",
    "with open('non_test_nr_multi_idr.csv') as f:\n",
    "    mult_idr_counter = 0\n",
    "    for line in f:\n",
    "        mult_idr_counter += 1\n",
    "print(mult_idr_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sing_no_idr_ind = set(np.random.choice(a=sing_no_idr_counter, size=300, replace=False))\n",
    "multi_no_idr_ind = set(np.random.choice(a=mult_no_idr_counter, size=5000, replace=False))\n",
    "sing_idr_ind = set(np.random.choice(a=sing_idr_counter, size=300, replace=False))\n",
    "multi_idr_ind = set(np.random.choice(a=mult_idr_counter, size=300, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n",
      "2011\n",
      "262\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "valid_sing_no_idr_arcs = set()\n",
    "valid_multi_no_idr_arcs = set()\n",
    "valid_sing_idr_arcs = set()\n",
    "valid_multi_idr_arcs = set()\n",
    "with open('non_test_nr_sing_no_idr.csv') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx in sing_no_idr_ind:\n",
    "            valid_sing_no_idr_arcs.add('|'.join([dom[0] for dom in parse_domain(line.split(',')[1])]))\n",
    "with open('non_test_nr_multi_no_idr.csv') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx in multi_no_idr_ind:\n",
    "            valid_multi_no_idr_arcs.add('|'.join([dom[0] for dom in parse_domain(line.split(',')[1])]))\n",
    "with open('non_test_nr_sing_idr.csv') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx in sing_idr_ind:\n",
    "            valid_sing_idr_arcs.add('|'.join([dom[0] for dom in parse_domain(line.split(',')[1])]))\n",
    "with open('non_test_nr_multi_idr.csv') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx in multi_idr_ind:\n",
    "            valid_multi_idr_arcs.add('|'.join([dom[0] for dom in parse_domain(line.split(',')[1])]))\n",
    "print(len(valid_sing_no_idr_arcs))\n",
    "print(len(valid_multi_no_idr_arcs))\n",
    "print(len(valid_sing_idr_arcs))\n",
    "print(len(valid_multi_idr_arcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2861"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sing_no_idr_doms = set(sum([arc.split('|') for arc in valid_sing_no_idr_arcs], []))\n",
    "valid_multi_no_idr_doms = set(sum([arc.split('|') for arc in valid_multi_no_idr_arcs], []))\n",
    "valid_sing_idr_doms = set(sum([arc.split('|') for arc in valid_sing_idr_arcs], []))\n",
    "valid_multi_idr_doms = set(sum([arc.split('|') for arc in valid_multi_idr_arcs], []))\n",
    "valid_doms = valid_sing_no_idr_doms.union(valid_multi_no_idr_doms).union(valid_sing_idr_doms).union(valid_multi_idr_doms)\n",
    "len(valid_doms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Line 0\n"
     ]
    }
   ],
   "source": [
    "valid_arcs = set()\n",
    "\n",
    "with open('non_test_nr_sing_no_idr.csv') as f:\n",
    "    with open('valid_set_unbalanced.csv', 'w') as fValid:\n",
    "        with open('non_valid.csv', 'a') as fNonValid:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx in sing_no_idr_ind:\n",
    "                    fValid.write(line)\n",
    "                    valid_arcs.add('|'.join([dom[0] for dom in parse_domain(line.split(',')[1])]))\n",
    "                else:\n",
    "                    fNonValid.write(line)\n",
    "                if idx % 1000000 == 0:\n",
    "                    clear_output()\n",
    "                    print(f'Processing Line {idx}')\n",
    "\n",
    "with open('non_test_nr_multi_no_idr.csv') as f:\n",
    "    with open('valid_set_unbalanced.csv', 'a') as fValid:\n",
    "        with open('non_valid.csv', 'a') as fNonValid:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx in multi_no_idr_ind:\n",
    "                    fValid.write(line)\n",
    "                    valid_arcs.add('|'.join([dom[0] for dom in parse_domain(line.split(',')[1])]))\n",
    "                else:\n",
    "                    fNonValid.write(line)\n",
    "                if idx % 1000000 == 0:\n",
    "                    clear_output()\n",
    "                    print(f'Processing Line {idx}')\n",
    "\n",
    "with open('non_test_nr_sing_idr.csv') as f:\n",
    "    with open('valid_set_unbalanced.csv', 'a') as fValid:\n",
    "        with open('non_valid.csv', 'a') as fNonValid:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx in sing_idr_ind:\n",
    "                    fValid.write(line)\n",
    "                    valid_arcs.add('|'.join([dom[0] for dom in parse_domain(line.split(',')[1])]))\n",
    "                else:\n",
    "                    fNonValid.write(line)\n",
    "                if idx % 1000000 == 0:\n",
    "                    clear_output()\n",
    "                    print(f'Processing Line {idx}')\n",
    "\n",
    "with open('non_test_nr_multi_idr.csv') as f:\n",
    "    with open('valid_set_unbalanced.csv', 'a') as fValid:\n",
    "        with open('non_valid.csv', 'a') as fNonValid:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx in multi_idr_ind:\n",
    "                    fValid.write(line)\n",
    "                    valid_arcs.add('|'.join([dom[0] for dom in parse_domain(line.split(',')[1])]))\n",
    "                else:\n",
    "                    fNonValid.write(line)\n",
    "                if idx % 1000000 == 0:\n",
    "                    clear_output()\n",
    "                    print(f'Processing Line {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('valid_set_unbalanced.csv', header=None).sample(frac=1).reset_index(drop=True).to_csv('valid_set_unbalanced.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Line 0\n",
      "[('PF16499|PF17801', 127), ('PF02563|PF10531', 53), ('PF20229|PF09828', 33), ('PF07584|PF13519', 31), ('PF06044|PF17726', 31), ('PF12633|PF01295', 30), ('PF00271|PF19833', 30), ('PF18984|PF18983', 29), ('PF16499|PF17450', 29), ('PF06218|PF06218', 28), ('PF05943|PF18945', 27), ('PF00906|PF00906', 27), ('PF17127|PF13905', 25), ('PF20066|PF19581', 23), ('PF12204|PF21053', 21), ('PF19346|PF11141', 20), ('PF06400|PF06401', 20), ('PF06268|PF06268|PF06268|PF06268', 19), ('PF03710|PF08335', 19), ('PF10568|PF17171', 18), ('PF05644|PF05644', 18), ('PF14908|PF18289', 17), ('PF17989|PF21522', 17), ('PF01074|PF09261', 16), ('PF12338|PF00101', 16), ('PF01507|PF11922', 16), ('PF00050|PF00050|PF00050', 15), ('PF13866|PF13867', 15), ('PF13657|PF07804', 15), ('PF10103|PF10103', 15), ('PF04820|PF04820', 15), ('PF01419|PF01419', 15), ('PF10703|PF17409', 15), ('PF04801|PF19725', 14), ('PF07653|PF07653', 14), ('PF04712|PF04712', 14), ('PF00088|PF00088', 14), ('PF19338|PF00243', 14), ('PF09414|PF18043', 13), ('PF13244|PF20501', 13), ('PF10408|PF04564', 13), ('PF12048|PF12048', 13), ('PF10558|PF10558', 13), ('PF18474|PF07000', 12), ('PF00508|d|PF00511', 12), ('PF04184|PF04184', 12), ('PF00421|PF00421', 12), ('PF14506|PF14507', 12), ('PF00050|PF00050|PF00050|PF00050|PF00050|PF00050', 11), ('PF14298|PF14298', 11), ('PF02326|PF06449', 11), ('PF02690|PF02690', 11), ('PF00182|PF00182', 11), ('PF00669|PF00700', 11), ('PF05634|PF05634', 11), ('PF13733|PF02709', 11), ('PF18053|PF00986', 11), ('PF20579|PF20579', 11), ('PF03492|PF03492', 11), ('PF02435|PF02435', 11), ('PF21985|PF08704', 11), ('PF08993|PF08994', 10), ('PF22555|PF22559', 10), ('PF02775|PF12367', 10), ('PF05023|PF09328', 10), ('PF18106|PF02486', 10), ('PF05158|PF05158', 10), ('PF01136|PF16325', 10), ('PF22916|PF06862', 10), ('PF14737|PF14740', 9), ('PF16990|PF02156', 9), ('PF20443|PF05609', 9), ('PF01419|PF01419|PF01419', 9), ('PF10390|d|PF07303', 9), ('PF12475|PF12433', 9), ('PF09307|PF08831|PF00086', 9), ('PF09507|PF09507', 9), ('PF03731|PF02735|PF03730|PF08785', 9), ('PF10057|PF10057', 8), ('PF03018|PF01419', 8), ('PF04673|PF04673', 8), ('PF09098|PF14930|PF09099|PF09100', 8), ('PF20996|PF17744', 8), ('PF10972|PF10972', 8), ('PF04818|d|PF16566', 8), ('PF04598|PF17708', 8), ('PF16408|PF17163|PF17165', 8), ('PF22019|PF02922', 8), ('PF03089|PF13341', 8), ('PF04466|PF17288', 8), ('PF14111|PF14392', 8), ('PF14266|PF14266', 8), ('PF10483|PF10483', 8), ('PF08300|PF08301|PF12941', 8), ('PF10139|PF10139', 8), ('PF14724|PF14724', 8), ('PF14863|PF14864', 8), ('PF11006|PF11006', 8), ('PF01122|PF14478', 8), ('PF01504|PF01504', 7)]\n",
      "2777\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "valid_arc_counts = []\n",
    "with open('valid_set_unbalanced.csv') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        valid_arc_counts.append('|'.join([dom[0] for dom in parse_domain(line.split(',')[1])]))\n",
    "        if idx % 1000000 == 0:\n",
    "            clear_output()\n",
    "            print(f'Processing Line {idx}')\n",
    "\n",
    "valid_arc_counts = Counter(valid_arc_counts)\n",
    "print(valid_arc_counts.most_common(100))\n",
    "print(len(valid_arc_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_arc_counter = {k:0 for k in valid_arc_counts.keys()}\n",
    "with open('valid_set_unbalanced.csv') as f:\n",
    "    with open('valid.csv', 'w') as fOut:\n",
    "        for line in f:\n",
    "            dom_arc_this_prot = '|'.join([dom[0] for dom in parse_domain(line.split(',')[1])])\n",
    "            if np.random.rand() < 0.8**valid_arc_counter[dom_arc_this_prot]:\n",
    "                fOut.write(line)\n",
    "                valid_arc_counter[dom_arc_this_prot] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2860\n"
     ]
    }
   ],
   "source": [
    "valid_doms = set()\n",
    "with open('valid.csv') as f:\n",
    "    for line in f:\n",
    "        prot_doms = [dom[0] for dom in parse_domain(line.split(',')[1]) if dom[0]!='d']\n",
    "        valid_doms.update(prot_doms)\n",
    "print(len(valid_doms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Line: 25000000\n"
     ]
    }
   ],
   "source": [
    "with open('non_valid.csv') as f:\n",
    "    with open('train_unbalanced.csv', 'w') as fOut:\n",
    "        for idx, line in enumerate(f):\n",
    "            if set([dom[0] for dom in parse_domain(line.split(',')[1]) if dom[0]!='d']).isdisjoint(valid_doms):\n",
    "                fOut.write(line)\n",
    "            if idx % 1000000 == 0:\n",
    "                clear_output()\n",
    "                print(f'Processing Line: {idx}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioInf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
